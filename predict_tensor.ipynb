{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1LpAeMPuDpe7llEgpcHxsbaRW9YlJ-2Gh",
      "authorship_tag": "ABX9TyMe9dYvsHrDqUlTjzOhwco3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Disciplined-22/LSTM_MODEL_PREDICT_WORD_1/blob/main/predict_tensor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "J-hxJd92oz9D"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The Tokenizer class in Keras is used for preparing text so it can be used in neural network models.\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "# Here, you're reading a text file located in your Google Drive.\n",
        "data = open('/content/drive/MyDrive/NLP_models/tensorflow_lstm/jim_rohn.txt').read()\n",
        "\n",
        "# The text data is converted to lowercase and split into lines.\n",
        "corpus = data.lower().split(\"\\n\")\n",
        "\n",
        "# The tokenizer is fit on the corpus. This updates the internal vocabulary based on the list of texts.\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "\n",
        "# The total number of words is the length of the word index plus one.\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Printing the word index gives you a dictionary of words and their assigned indices.\n",
        "print(tokenizer.word_index)\n",
        "\n",
        "# Printing total_words gives you the total number of unique words in the corpus.\n",
        "print(total_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fcRw-KBtDlr",
        "outputId": "f29c6a5f-e377-4459-d421-b503e3a5e418"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'the': 1, 'you': 2, 'to': 3, 'is': 4, 'your': 5, 'a': 6, 'make': 7, \"don't\": 8, 'of': 9, 'not': 10, 'be': 11, 'will': 12, 'success': 13, 'it': 14, 'what': 15, 'in': 16, 'for': 17, 'more': 18, 'life': 19, 'better': 20, 'only': 21, 'are': 22, 'do': 23, 'today': 24, 'and': 25, 'that': 26, 'get': 27, \"it's\": 28, 'by': 29, 'become': 30, 'future': 31, 'can': 32, 'yourself': 33, 'wish': 34, 'education': 35, 'count': 36, 'create': 37, 'about': 38, 'self': 39, 'goals': 40, 'things': 41, 'on': 42, 'just': 43, 'work': 44, 'person': 45, 'were': 46, 'between': 47, 'our': 48, 'living': 49, 'days': 50, 'best': 51, 'way': 52, 'getting': 53, 'have': 54, 'destination': 55, 'easier': 56, 'key': 57, 'discipline': 58, 'bridge': 59, 'major': 60, 'difference': 61, 'set': 62, 'goal': 63, 'tomorrow': 64, 'formal': 65, 'fortune': 66, 'less': 67, 'failure': 68, 'predict': 69, 'determines': 70, 'learn': 71, 'reflection': 72, 'them': 73, 'wisely': 74, 'wait': 75, 'investment': 76, 'income': 77, 'place': 78, 'give': 79, 'could': 80, 'i': 81, 'pursued': 82, 'attracted': 83, 'improvement': 84, 'accomplishment': 85, 'minor': 86, 'focus': 87, 'few': 88, 'big': 89, 'does': 90, 'chance': 91, 'gets': 92, 'change': 93, 'millionaire': 94, 'limit': 95, 'realization': 96, 'doubts': 97, 'know': 98, 'need': 99, 'say': 100, 'harvest': 101, 'seeds': 102, 'plant': 103, 'excuses': 104, 'nails': 105, 'used': 106, 'build': 107, 'house': 108, 'attitude': 109, 'altitude': 110, 'doing': 111, 'ordinary': 112, 'extraordinarily': 113, 'well': 114, 'earn': 115, 'thoughts': 116, 'choose': 117, 'opportunity': 118, 'greatest': 119, 'secret': 120, 'ahead': 121, 'started': 122, 'grow': 123, 'extent': 124, 'making': 125, 'fear': 126, 'journey': 127, 'follower': 128, 'student': 129, 'determine': 130, 'where': 131, 'harder': 132, 'than': 133, 'grateful': 134, 'road': 135, 'maps': 136, 'guide': 137, 'who': 138, 'seldom': 139, 'exceed': 140, 'personal': 141, 'development': 142, 'ask': 143, 'level': 144, 'learning': 145, 'money': 146, \"you'll\": 147, 'never': 148, 'back': 149, 'being': 150, 'same': 151, 'next': 152, 'year': 153, 'as': 154, 'must': 155, 'like': 156, 'bank': 157, 'account': 158, 'invest': 159, 'limits': 160, 'exist': 161, 'ones': 162, 'destined': 163, 'decide': 164, 'take': 165, 'care': 166, 'body': 167, 'live': 168, 'reaching': 169, 'top': 170, 'helping': 171, 'others': 172, 'climb': 173, 'present': 174, 'circumstances': 175, 'go': 176, 'they': 177, 'merely': 178, 'start': 179, 'job': 180, 'attract': 181, 'into': 182, 'measurable': 183, 'progress': 184, 'reasonable': 185, 'time': 186, \"can't\": 187, 'hire': 188, 'someone': 189, 'else': 190, 'push': 191, 'ups': 192, 'fewer': 193, 'challenges': 194, 'wisdom': 195, 'show': 196, 'possible': 197, 'if': 198, 'want': 199, 'wealthy': 200, 'study': 201, 'wealth': 202, 'great': 203, 'love': 204, 'settle': 205, 'share': 206, 'receive': 207, 'habits': 208, 'wind': 209, 'sail': 210, 'afraid': 211, 'believe': 212, 'worth': 213, 'belief': 214, 'help': 215, 'fact': 216, 'thing': 217, 'standing': 218, 'story': 219, 'keep': 220, 'telling': 221, 'accomplishments': 222, 'rest': 223, 'necessity': 224, 'an': 225, 'objective': 226, 'hard': 227, 'smart': 228, 'solve': 229, 'any': 230, 'problem': 231, 'here': 232, 'three': 233, 'questions': 234, 'first': 235, 'second': 236, 'read': 237, 'third': 238, 'directly': 239, 'influences': 240, 'strive': 241, 'let': 242, 'from': 243, 'own': 244, 'experiences': 245, 'ever': 246, 'created': 247, 'complain': 248}\n",
            "249\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating an empty list to store input sequences\n",
        "input_sequences = []\n",
        "\n",
        "# Iterating through each line in the corpus\n",
        "for line in corpus:\n",
        "    # Converting the current line to a sequence of tokens using the tokenizer\n",
        "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "\n",
        "    # Generating n-gram sequences from the token list\n",
        "    for i in range(1, len(token_list)):\n",
        "        n_gram_sequence = token_list[:i+1]\n",
        "\n",
        "        # Appending the n-gram sequence to the list of input sequences\n",
        "        input_sequences.append(n_gram_sequence)\n",
        "\n",
        "\n",
        "# pad sequences\n",
        "max_sequence_len = max([len(x) for x in input_sequences])\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "\n",
        "# create predictors and label\n",
        "xs, labels = input_sequences[:,:-1],input_sequences[:,-1]\n",
        "\n",
        "#used to covert labels to one hot encode in order to get the ys\n",
        "ys = tf.keras.utils.to_categorical(labels, num_classes=total_words)"
      ],
      "metadata": {
        "id": "kwPZ5-nsx4Eu"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "\n",
        "# Defining the Model\n",
        "model = Sequential()\n",
        "\n",
        "# Adding an Embedding layer with input dimension total_words, embedding dimension 100,\n",
        "# and input length max_sequence_len-1\n",
        "model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\n",
        "\n",
        "# Uncomment the following lines and use Bidirectional if necessary\n",
        "# model.add(Bidirectional(LSTM(150)))\n",
        "\n",
        "# Adding a unidirectional LSTM layer with 150 units\n",
        "model.add(LSTM(200))\n",
        "\n",
        "# Adding a Dense output layer with total_words units and softmax activation\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "adam = Adam(lr=0.01)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
        "#earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=0, mode='auto')\n",
        "history = model.fit(xs, ys, epochs=100, verbose=1)\n",
        "#print model.summary()\n",
        "print(model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z-PXbX7G7b0e",
        "outputId": "1b5bf0b3-9014-4e7c-8a33-25e6b1e0a020"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "29/29 [==============================] - 4s 89ms/step - loss: 5.2423 - accuracy: 0.0465\n",
            "Epoch 2/100\n",
            "29/29 [==============================] - 1s 47ms/step - loss: 4.8923 - accuracy: 0.0520\n",
            "Epoch 3/100\n",
            "29/29 [==============================] - 1s 32ms/step - loss: 4.8314 - accuracy: 0.0509\n",
            "Epoch 4/100\n",
            "29/29 [==============================] - 1s 27ms/step - loss: 4.7970 - accuracy: 0.0664\n",
            "Epoch 5/100\n",
            "29/29 [==============================] - 0s 6ms/step - loss: 4.7490 - accuracy: 0.0819\n",
            "Epoch 6/100\n",
            "29/29 [==============================] - 0s 16ms/step - loss: 4.6623 - accuracy: 0.1117\n",
            "Epoch 7/100\n",
            "29/29 [==============================] - 0s 12ms/step - loss: 4.4928 - accuracy: 0.1162\n",
            "Epoch 8/100\n",
            "29/29 [==============================] - 0s 9ms/step - loss: 4.2557 - accuracy: 0.1659\n",
            "Epoch 9/100\n",
            "29/29 [==============================] - 0s 12ms/step - loss: 3.9640 - accuracy: 0.1980\n",
            "Epoch 10/100\n",
            "29/29 [==============================] - 0s 9ms/step - loss: 3.6495 - accuracy: 0.2323\n",
            "Epoch 11/100\n",
            "29/29 [==============================] - 0s 9ms/step - loss: 3.3346 - accuracy: 0.2821\n",
            "Epoch 12/100\n",
            "29/29 [==============================] - 0s 13ms/step - loss: 3.0278 - accuracy: 0.3186\n",
            "Epoch 13/100\n",
            "29/29 [==============================] - 0s 16ms/step - loss: 2.7434 - accuracy: 0.4004\n",
            "Epoch 14/100\n",
            "29/29 [==============================] - 0s 15ms/step - loss: 2.4821 - accuracy: 0.4646\n",
            "Epoch 15/100\n",
            "29/29 [==============================] - 0s 13ms/step - loss: 2.2281 - accuracy: 0.5243\n",
            "Epoch 16/100\n",
            "29/29 [==============================] - 0s 11ms/step - loss: 1.9985 - accuracy: 0.5896\n",
            "Epoch 17/100\n",
            "29/29 [==============================] - 0s 12ms/step - loss: 1.7862 - accuracy: 0.6250\n",
            "Epoch 18/100\n",
            "29/29 [==============================] - 0s 16ms/step - loss: 1.5974 - accuracy: 0.6737\n",
            "Epoch 19/100\n",
            "29/29 [==============================] - 0s 9ms/step - loss: 1.4386 - accuracy: 0.6991\n",
            "Epoch 20/100\n",
            "29/29 [==============================] - 0s 12ms/step - loss: 1.2827 - accuracy: 0.7345\n",
            "Epoch 21/100\n",
            "29/29 [==============================] - 0s 9ms/step - loss: 1.1528 - accuracy: 0.7777\n",
            "Epoch 22/100\n",
            "29/29 [==============================] - 0s 9ms/step - loss: 1.0458 - accuracy: 0.8108\n",
            "Epoch 23/100\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.9529 - accuracy: 0.8175\n",
            "Epoch 24/100\n",
            "29/29 [==============================] - 0s 12ms/step - loss: 0.8633 - accuracy: 0.8473\n",
            "Epoch 25/100\n",
            "29/29 [==============================] - 0s 8ms/step - loss: 0.7969 - accuracy: 0.8584\n",
            "Epoch 26/100\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.7212 - accuracy: 0.8673\n",
            "Epoch 27/100\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.6651 - accuracy: 0.8783\n",
            "Epoch 28/100\n",
            "29/29 [==============================] - 0s 8ms/step - loss: 0.6237 - accuracy: 0.8794\n",
            "Epoch 29/100\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.5813 - accuracy: 0.8872\n",
            "Epoch 30/100\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.5450 - accuracy: 0.8905\n",
            "Epoch 31/100\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.5089 - accuracy: 0.8938\n",
            "Epoch 32/100\n",
            "29/29 [==============================] - 0s 9ms/step - loss: 0.4821 - accuracy: 0.8938\n",
            "Epoch 33/100\n",
            "29/29 [==============================] - 0s 11ms/step - loss: 0.4561 - accuracy: 0.9027\n",
            "Epoch 34/100\n",
            "29/29 [==============================] - 0s 9ms/step - loss: 0.4407 - accuracy: 0.9004\n",
            "Epoch 35/100\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.4171 - accuracy: 0.9071\n",
            "Epoch 36/100\n",
            "29/29 [==============================] - 0s 9ms/step - loss: 0.3985 - accuracy: 0.9126\n",
            "Epoch 37/100\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.3858 - accuracy: 0.9104\n",
            "Epoch 38/100\n",
            "29/29 [==============================] - 0s 9ms/step - loss: 0.3774 - accuracy: 0.9071\n",
            "Epoch 39/100\n",
            "29/29 [==============================] - 0s 6ms/step - loss: 0.3670 - accuracy: 0.9038\n",
            "Epoch 40/100\n",
            "29/29 [==============================] - 0s 6ms/step - loss: 0.3527 - accuracy: 0.9071\n",
            "Epoch 41/100\n",
            "29/29 [==============================] - 0s 8ms/step - loss: 0.3429 - accuracy: 0.9126\n",
            "Epoch 42/100\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.3320 - accuracy: 0.9126\n",
            "Epoch 43/100\n",
            "29/29 [==============================] - 0s 8ms/step - loss: 0.3229 - accuracy: 0.9093\n",
            "Epoch 44/100\n",
            "29/29 [==============================] - 0s 13ms/step - loss: 0.3137 - accuracy: 0.9082\n",
            "Epoch 45/100\n",
            "29/29 [==============================] - 0s 17ms/step - loss: 0.3091 - accuracy: 0.9115\n",
            "Epoch 46/100\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.3001 - accuracy: 0.9093\n",
            "Epoch 47/100\n",
            "29/29 [==============================] - 0s 13ms/step - loss: 0.2986 - accuracy: 0.9093\n",
            "Epoch 48/100\n",
            "29/29 [==============================] - 0s 8ms/step - loss: 0.2948 - accuracy: 0.9104\n",
            "Epoch 49/100\n",
            "29/29 [==============================] - 0s 8ms/step - loss: 0.2882 - accuracy: 0.9093\n",
            "Epoch 50/100\n",
            "29/29 [==============================] - 0s 9ms/step - loss: 0.2825 - accuracy: 0.9137\n",
            "Epoch 51/100\n",
            "29/29 [==============================] - 0s 8ms/step - loss: 0.2788 - accuracy: 0.9104\n",
            "Epoch 52/100\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.2784 - accuracy: 0.9004\n",
            "Epoch 53/100\n",
            "29/29 [==============================] - 0s 8ms/step - loss: 0.2742 - accuracy: 0.9071\n",
            "Epoch 54/100\n",
            "29/29 [==============================] - 0s 8ms/step - loss: 0.2702 - accuracy: 0.9082\n",
            "Epoch 55/100\n",
            "29/29 [==============================] - 0s 8ms/step - loss: 0.2679 - accuracy: 0.9104\n",
            "Epoch 56/100\n",
            "29/29 [==============================] - 0s 8ms/step - loss: 0.2713 - accuracy: 0.9038\n",
            "Epoch 57/100\n",
            "29/29 [==============================] - 0s 8ms/step - loss: 0.2623 - accuracy: 0.9093\n",
            "Epoch 58/100\n",
            "29/29 [==============================] - 0s 10ms/step - loss: 0.2611 - accuracy: 0.9093\n",
            "Epoch 59/100\n",
            "29/29 [==============================] - 0s 6ms/step - loss: 0.2584 - accuracy: 0.9038\n",
            "Epoch 60/100\n",
            "29/29 [==============================] - 0s 12ms/step - loss: 0.2597 - accuracy: 0.9093\n",
            "Epoch 61/100\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.2542 - accuracy: 0.9093\n",
            "Epoch 62/100\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.2535 - accuracy: 0.9071\n",
            "Epoch 63/100\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.2563 - accuracy: 0.9137\n",
            "Epoch 64/100\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.2545 - accuracy: 0.9115\n",
            "Epoch 65/100\n",
            "29/29 [==============================] - 0s 9ms/step - loss: 0.2480 - accuracy: 0.9126\n",
            "Epoch 66/100\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.2492 - accuracy: 0.9015\n",
            "Epoch 67/100\n",
            "29/29 [==============================] - 0s 6ms/step - loss: 0.2436 - accuracy: 0.9093\n",
            "Epoch 68/100\n",
            "29/29 [==============================] - 0s 13ms/step - loss: 0.2437 - accuracy: 0.9082\n",
            "Epoch 69/100\n",
            "29/29 [==============================] - 0s 11ms/step - loss: 0.2423 - accuracy: 0.9126\n",
            "Epoch 70/100\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.2430 - accuracy: 0.9038\n",
            "Epoch 71/100\n",
            "29/29 [==============================] - 0s 6ms/step - loss: 0.2414 - accuracy: 0.9082\n",
            "Epoch 72/100\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.2416 - accuracy: 0.9093\n",
            "Epoch 73/100\n",
            "29/29 [==============================] - 0s 6ms/step - loss: 0.2421 - accuracy: 0.9004\n",
            "Epoch 74/100\n",
            "29/29 [==============================] - 0s 9ms/step - loss: 0.2406 - accuracy: 0.9104\n",
            "Epoch 75/100\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.2393 - accuracy: 0.9104\n",
            "Epoch 76/100\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.2395 - accuracy: 0.9082\n",
            "Epoch 77/100\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.2393 - accuracy: 0.9115\n",
            "Epoch 78/100\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.2365 - accuracy: 0.9093\n",
            "Epoch 79/100\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.2365 - accuracy: 0.9082\n",
            "Epoch 80/100\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.2401 - accuracy: 0.9049\n",
            "Epoch 81/100\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.2398 - accuracy: 0.9027\n",
            "Epoch 82/100\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.2363 - accuracy: 0.9071\n",
            "Epoch 83/100\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.2297 - accuracy: 0.9082\n",
            "Epoch 84/100\n",
            "29/29 [==============================] - 0s 11ms/step - loss: 0.2289 - accuracy: 0.9071\n",
            "Epoch 85/100\n",
            "29/29 [==============================] - 0s 8ms/step - loss: 0.2316 - accuracy: 0.9071\n",
            "Epoch 86/100\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.2294 - accuracy: 0.9104\n",
            "Epoch 87/100\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.2263 - accuracy: 0.9071\n",
            "Epoch 88/100\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.2295 - accuracy: 0.9071\n",
            "Epoch 89/100\n",
            "29/29 [==============================] - 0s 6ms/step - loss: 0.2303 - accuracy: 0.9093\n",
            "Epoch 90/100\n",
            "29/29 [==============================] - 0s 6ms/step - loss: 0.2309 - accuracy: 0.9071\n",
            "Epoch 91/100\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.2253 - accuracy: 0.9093\n",
            "Epoch 92/100\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.2281 - accuracy: 0.9027\n",
            "Epoch 93/100\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.2309 - accuracy: 0.9115\n",
            "Epoch 94/100\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.2291 - accuracy: 0.9093\n",
            "Epoch 95/100\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.2254 - accuracy: 0.9049\n",
            "Epoch 96/100\n",
            "29/29 [==============================] - 0s 6ms/step - loss: 0.2249 - accuracy: 0.9038\n",
            "Epoch 97/100\n",
            "29/29 [==============================] - 0s 6ms/step - loss: 0.2236 - accuracy: 0.9071\n",
            "Epoch 98/100\n",
            "29/29 [==============================] - 0s 6ms/step - loss: 0.2261 - accuracy: 0.9115\n",
            "Epoch 99/100\n",
            "29/29 [==============================] - 0s 6ms/step - loss: 0.2315 - accuracy: 0.9093\n",
            "Epoch 100/100\n",
            "29/29 [==============================] - 0s 6ms/step - loss: 0.2244 - accuracy: 0.9004\n",
            "<keras.src.engine.sequential.Sequential object at 0x7a35f01e76a0>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting the initial seed text and specifying the number of words to predict\n",
        "seed_text = \"History is the proof the more you give\"\n",
        "next_words = 10\n",
        "\n",
        "# Generating the next words in the sequence\n",
        "for _ in range(next_words):\n",
        "    # Converting the seed text to a sequence of tokens using the tokenizer\n",
        "    token_list_1 = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "\n",
        "    # Padding the token sequence to match the model's input length\n",
        "    token_list_2 = pad_sequences([token_list_1], maxlen=max_sequence_len-1, padding='pre')\n",
        "\n",
        "    # Predicting the next word using the trained model\n",
        "    predicted = np.argmax(model.predict(token_list_2), axis=-1)\n",
        "\n",
        "    # Converting the predicted index to the corresponding word\n",
        "    output_word = \"\"\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == predicted:\n",
        "            output_word = word\n",
        "            break\n",
        "\n",
        "    # Appending the predicted word to the seed text for the next iteration\n",
        "    seed_text += \" \" + output_word\n",
        "\n",
        "# Displaying the generated sequence\n",
        "print(seed_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZAYYqbQpCJt4",
        "outputId": "0eac8202-fb18-421f-9f67-f253f40bdc00"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 472ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "History is the proof the more you give the more you receive into your life to be grateful\n"
          ]
        }
      ]
    }
  ]
}